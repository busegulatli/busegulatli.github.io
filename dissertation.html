<!doctype html>
<html>
<style>
@import url('https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Lato:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap');
</style> 




<head>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
	
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
	<link rel="stylesheet" href="buttons/suit_and_tie_button.css">
	<link rel="stylesheet" href="buttons/download_button.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <meta name="viewport" content="width=device-width">
	<meta name="google-site-verification" content="CXW17yCRmsHT-UKe8B86505WLbEk21wxSEEs20nAokY" />
	

	<!-- Favicon head tag -->
	<link rel="icon" href="argonne.png" type="image/x-icon">
	

</head>
  <!----------------------------------------------------------------------------------------->
  <!--THIS IS THE LEFT COLUMN -->
  <body>
	<!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-N98HVV6"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

<!--THIS IS THE LEFT COLUMN -->
  <div class="wrapper">
	<header>	  
	
	<h2>Buse Gul Atli Tekgul</h2>
	
	<img align="middle" src="profilephoto.jpg" width="300" height=auto />
	
	<!--<img align="middle" src="https://lh3.googleusercontent.com/d/1CTcz7LnEpjcJWvPr4O5Hlln-OfGr1BEz" width="300" height=auto />
	-->
	
	<p></p>

	<!--<h2>Navigation</h2>-->
	<!--<hr>-->

	<nh><p><a target="_self" href="index.html"><b>Home</b></a></p></nh>
	<nh><p><a target="_self" href="teaching.html"><b>Teaching</b></a></p></nh>
		<nh><p><a target="_self" href="talks.html"><b>Talks</b></a></p></nh>
	<nh><p><a target="_self" href="publications.html"><b>Publications</b></a></p></nh>

	<hr>
  <h3>Other platforms</h3>

  <nh><p><a target="_self" href="https://scholar.google.com.tr/citations?user=ly-bZz0AAAAJ&hl=en"><b>Google Scholar <i class='fa fa-google'></i> </b></a></p></nh>
	  <nh><p><a target="_self" href="https://github.com/bussfromspace"><b>GitHub <i class='fa fa-github'></i> </b></a></p></nh>
	  <nh><p><a target="_self" href="https://fi.linkedin.com/in/buse-gul-atli-18821298/en"><b>LinkedIn <i class='fa fa-linkedin'></i> </b></a></p></nh>
		
	</header>

  
	  
	  <!----------------------------------------------------------------------------------------->
	  <!--THIS IS THE RIGHT COLUMN -->
	  <br><br>
      <section>	
		<h2>Doctoral Dissertation</h2>
		<h3 id="securing-machine-learning:-streamlining-attacks-and-defenses-under-realistic-adversary-models">Securing Machine Learning: Streamlining Attacks and Defenses Under Realistic Adversary Models</h3>
		<p>Over the last decade, machine learning (ML) and artificial intelligence (AI) solutions have been widely used in many applications, due to their remarkable performance in various domains.
			Over the last decade, machine learning (ML) and artificial intelligence (AI) solutions have been widely used in many applications, due to their remarkable performance in various domains.
                        However, the rapid progress in AI and ML driven systems induces new security risks associated with the entire ML pipeline and vulnerabilities that can be exploited by adversaries to attack these systems.
			Although different defense methods have been proposed to prevent, mitigate, or detect attacks, they are either limited or ineffective against continuously evolving attack strategies due to the increased availability of ML tools, databases, and computing resources. 
			For instance, evasion attacks that aim to fool ML systems have become a major threat to security or safety-critical applications. Model extraction attacks, which attempt to compromise the confidentiality of ML models, are also an important concern. 
			Both these attacks pose serious threats even if the ML model is deployed behind an application program interface (API) and does not expose any information about the model itself to end users.<br>

			This dissertation investigates the security threats to ML systems caused by evasion and model extraction attacks. 
			This dissertation contains three parts, namely, adversarial examples in ML, ownership verification in ML, and model extraction as a realistic threat. 
			In the first part, we develop evasion attacks, which attain high effectiveness and efficiency at the same time, on image classifiers and deep reinforcement learning (DRL) agents. 
			In both applications, we place a particular focus on operating within realistic adversary models. We show that our evasion attack on image classifiers can be as effective as state-of-the-art attacks with a cost decreased by three orders of magnitude.
			We also demonstrate that we can destroy the performance of DRL agents with a small online cost and without modifying their inner states.
			In the second part, we propose a novel approach that integrates ML watermarking solutions into the federated learning process with low computational (+3.2%) overhead and negligible degradation in the model performance (-0.17%). 
			We also demonstrate that different dataset tracing and watermarking methods can only reliably demonstrate the ownership of big datasets having a high number of classes (&ge; 30), and with limited adversarial capabilities. 
			In the third part, we show that the effectiveness of state-of-the-art model extraction attacks is affected by several aspects such as the amount of information delivered via the API for each input, or the adversary's knowledge about the task and ML model architecture. 
			We also develop alternative watermarking techniques that can survive during model extraction attacks and deter adversaries by increasing the cost of the attack. 
			The findings in this dissertation will help ML model owners evaluate potential vulnerabilities and remedies against model evasion and extraction attacks considering different security requirements and realistic adversary models.
			<hr>
			<h3 id="publications-included-in-the-dissertation">Publications included in the dissertation </h3>
			<h3 id="real-time-adversarial-perturbations-against-deep-reinforcement-learning-policies-attacks-and-defenses">Real-time Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks and Defenses</h3>
		<p><strong>Buse G. A. Tekgul</strong>, Shelly Wang, Samuel Marchal, N Asokan <br>
		<a href="https://arxiv.org/abs/2106.08746">ArXiv</a>, Please email for the code.</p>
		<h3 id="on-the-effectiveness-of-dataset-watermarking-in-adversarial-settings">On the Effectiveness of Dataset Watermarking in Adversarial Settings</h3>
		<p>CODASPY-IWSPA'22, 8th ACM International Workshop on Security and Privacy Analytics <br>
		<strong>Buse G. A. Tekgul</strong>, N. Asokan <br>
		<a href="https://arxiv.org/abs/2202.12506">ArXiv</a>, Please email for the code.</p>
		<h3 id="waffle-watermarking-in-federated-learning">WAFFLE: Watermarking in Federated Learning</h3>
		<p>SRDS'21, The 40th International Symposium on Reliable Distributed Systems <br>
		<strong>Buse G. A. Tekgul</strong>, Yuxi Xia, Samuel Marchal, N. Asokan <br>
		<a href="https://ieeexplore.ieee.org/document/9603498">Paper</a> <a href="https://arxiv.org/abs/2008.07298">ArXiv</a> <a href="https://github.com/ssg-research/WAFFLE">Code</a></p>
		<h3 id="dawn-dynamic-adversarial-watermarking-of-neural-networks">DAWN: Dynamic Adversarial Watermarking of Neural Networks</h3>
		<p>ACMMM'21, 29th ACM Conference on Multimedia <br>
		Sebastian Szyller, <strong>Buse G. A. Tekgul</strong>, Samuel Marchal, N Asokan <br>
		<a href="https://dl.acm.org/doi/10.1145/3474085.3475591">Paper</a> <a href="https://arxiv.org/abs/1906.00830">ArXiv</a> <a href="https://github.com/ssg-research/dawn-dynamic-adversarial-watermarking-of-neural-networks">Code</a></p>
		<h3 id="extraction-of-complex-dnn-models-real-threat-or-boogeyman">Extraction of Complex DNN Models: Real Threat or Boogeyman?</h3>
		<p>AAAI-EDSMLS'20, International Workshop on Engineering Dependable and Secure Machine Learning Systems <br>
		<strong>Buse G. A. Tekgul</strong>, Sebastian Szyller, Mika Juuti, Samuel Marchal, N. Asokan <br>
		<a href="https://link.springer.com/chapter/10.1007/978-3-030-62144-5_4">Paper</a> <a href="https://arxiv.org/abs/1910.05429">ArXiv</a>, Please email for the code.
		<a href="https://ssg.aalto.fi/wp-content/uploads/2019/12/EDSMLS_presentation.pdf">Slides</a></p>
		<h3 id="making-targeted-black-box-evasion-attacks-effective-and-efficient">Making Targeted Black-box Evasion Attacks Effective and Efficient</h3>
		<p>AISec'19, 12th ACM Workshop on Artificial Intelligence and Security <br>
		Mika Juuti, <strong>Buse G. A. Tekgul</strong>, N. Asokan <br>
		<a href="https://dl-acm-org.libproxy.aalto.fi/doi/10.1145/3338501.3357366">Paper</a> <a href="https://arxiv.org/abs/1906.03397">ArXiv</a>, Please email for the code.
		<a href="https://ssg.aalto.fi/wp-content/uploads/2020/04/AISec2019.pdf">Slides</a></p>
		<p>Aalto <a href="buttons/buttons/dnn.zip" title="University" target="_blank"><b>here.</b></a></p>
		</p>
		
  </body>
  
  <!-- Java script for abstract button -->
<script>
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    } 
  });
}

</script>

<script>

$(document).ready(function() {
  $('button#button').click(function() {
    $(this).toggleClass("down");
  });
});
</script>

</html>
